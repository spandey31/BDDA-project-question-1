{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform NLP text classification on corona virus tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Login to virtual machine \n",
    "2) Go to LXTerminal \n",
    "3) To start hadoop write ./allstart.sh \n",
    "4) Copy the required csv file to the local system through bitwise client \n",
    "5) Once hadoop gets started use command hadoop fs -put Corona_NLP_train.csv to import the file to hadoop\n",
    "6) After hadoop gets started write command pysparknb to start pyspark 7) In pyspark take a jupyter notebook and start with the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark= SparkSession.builder.appName('nlp').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the necessary libraries\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the dataset\n",
    "corona= spark.read.csv('Corona_NLP_train.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(UserName='3799', ScreenName='48751', Location='London', TweetAt='16-03-2020', OriginalTweet='@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://t.co/I2NlzdxNo8', Sentiment='Neutral')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corona.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "|            UserName|          ScreenName|            Location|             TweetAt|       OriginalTweet|Sentiment|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "|                3799|               48751|              London|          16-03-2020|@MeNyrbie @Phil_G...|  Neutral|\n",
      "|                3800|               48752|                  UK|          16-03-2020|advice Talk to yo...| Positive|\n",
      "|                3801|               48753|           Vagabonds|          16-03-2020|Coronavirus Austr...| Positive|\n",
      "|                3802|               48754|                null|          16-03-2020|My food stock is ...|     null|\n",
      "|              PLEASE|         don't panic| THERE WILL BE EN...|                null|                null|     null|\n",
      "|           Stay calm|          stay safe.|                null|                null|                null|     null|\n",
      "|#COVID19france #C...|            Positive|                null|                null|                null|     null|\n",
      "|                3803|               48755|                null|          16-03-2020|Me, ready to go a...|     null|\n",
      "|Not because I'm p...| but because my f...|          but please| don't panic. It ...|                null|     null|\n",
      "|#CoronavirusFranc...|  Extremely Negative|                null|                null|                null|     null|\n",
      "|                3804|               48756|ÃœT: 36.319708,-82...|          16-03-2020|As news of the re...| Positive|\n",
      "|                3805|               48757|35.926541,-78.753267|          16-03-2020|\"Cashier at groce...| Positive|\n",
      "|                3806|               48758|             Austria|          16-03-2020|Was at the superm...|     null|\n",
      "|#toiletpapercrisi...|             Neutral|                null|                null|                null|     null|\n",
      "|                3807|               48759|     Atlanta, GA USA|          16-03-2020|Due to COVID-19 o...| Positive|\n",
      "|                3808|               48760|    BHAVNAGAR,GUJRAT|          16-03-2020|For corona preven...| Negative|\n",
      "|                3809|               48761|      Makati, Manila|          16-03-2020|All month there h...|  Neutral|\n",
      "|                3810|               48762|Pitt Meadows, BC,...|          16-03-2020|Due to the Covid-...|     null|\n",
      "|The wait time may...| particularly bee...|                null|                null|                null|     null|\n",
      "|We thank you for ...|  Extremely Positive|                null|                null|                null|     null|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corona.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|       OriginalTweet|Sentiment|\n",
      "+--------------------+---------+\n",
      "|@MeNyrbie @Phil_G...|  Neutral|\n",
      "|advice Talk to yo...| Positive|\n",
      "|Coronavirus Austr...| Positive|\n",
      "|My food stock is ...|     null|\n",
      "|                null|     null|\n",
      "|                null|     null|\n",
      "|                null|     null|\n",
      "|Me, ready to go a...|     null|\n",
      "|                null|     null|\n",
      "|                null|     null|\n",
      "+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corona.select(\"OriginalTweet\", \"Sentiment\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing unnecesary columns\n",
    "drop_col= ['ScreenName', 'UserName', 'TweetAt','Location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corona= corona.select([column for column in corona.columns if column not in drop_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|       OriginalTweet|Sentiment|\n",
      "+--------------------+---------+\n",
      "|@MeNyrbie @Phil_G...|  Neutral|\n",
      "|advice Talk to yo...| Positive|\n",
      "|Coronavirus Austr...| Positive|\n",
      "|My food stock is ...|     null|\n",
      "|                null|     null|\n",
      "|                null|     null|\n",
      "|                null|     null|\n",
      "|Me, ready to go a...|     null|\n",
      "|                null|     null|\n",
      "|                null|     null|\n",
      "|As news of the re...| Positive|\n",
      "|\"Cashier at groce...| Positive|\n",
      "|Was at the superm...|     null|\n",
      "|                null|     null|\n",
      "|Due to COVID-19 o...| Positive|\n",
      "|For corona preven...| Negative|\n",
      "|All month there h...|  Neutral|\n",
      "|Due to the Covid-...|     null|\n",
      "|                null|     null|\n",
      "|                null|     null|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corona.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- OriginalTweet: string (nullable = true)\n",
      " |-- Sentiment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corona.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28617"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dropping null values from dataset\n",
    "corona = corona.dropna()\n",
    "corona.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|           Sentiment|count|\n",
      "+--------------------+-----+\n",
      "|            Positive| 7718|\n",
      "|            Negative| 6857|\n",
      "|             Neutral| 5224|\n",
      "|  Extremely Positive| 4412|\n",
      "|  Extremely Negative| 3751|\n",
      "|   social distancing|    5|\n",
      "|    N. Y. - April 10|    3|\n",
      "| state governors ...|    2|\n",
      "|             however|    2|\n",
      "| supermarket workers|    2|\n",
      "|        Stay with us|    2|\n",
      "| but we also need...|    2|\n",
      "| or click the lin...|    2|\n",
      "| just \"\"selfish p...|    2|\n",
      "|           of course|    2|\n",
      "| not going to the...|    2|\n",
      "| ecological collapse|    2|\n",
      "|        Corona Virus|    2|\n",
      "|            delivery|    2|\n",
      "| Big Tech could b...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking the type of sentiments\n",
    "from pyspark.sql.functions import col\n",
    "corona.groupBy(\"Sentiment\").count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|         Sentiment|count|\n",
      "+------------------+-----+\n",
      "|          Positive| 7718|\n",
      "|          Negative| 6857|\n",
      "|           Neutral| 5224|\n",
      "|Extremely Positive| 4412|\n",
      "|Extremely Negative| 3751|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Filtering the data to get Sentiment in terms of Positive, Negative,Neutral,Extremely Positive and Extremely Negative values\n",
    "import pyspark.sql.functions as fn\n",
    "data = corona.where(fn.col(\"Sentiment\").isin([\"Positive\", \"Negative\", \"Neutral\",\"Extremely Positive\",\"Extremely Negative\"]))\n",
    "data.groupBy(\"Sentiment\").count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data has been transformed with the help of tokenizers, stopwords to find the count vector of the feature column for the purpose of modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "# using tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"OriginalTweet\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# checking the stop words\n",
    "add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"] \n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "# checking the bag of words count\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=1000, minDF=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "String indexing has been done to convert the text to label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+--------------------+--------------------+-----+\n",
      "|       OriginalTweet|         Sentiment|               words|            filtered|            features|label|\n",
      "+--------------------+------------------+--------------------+--------------------+--------------------+-----+\n",
      "|@MeNyrbie @Phil_G...|           Neutral|[menyrbie, phil_g...|[menyrbie, phil_g...|(1000,[1,5],[2.0,...|  2.0|\n",
      "|advice Talk to yo...|          Positive|[advice, talk, to...|[advice, talk, to...|(1000,[0,2,25,34,...|  0.0|\n",
      "|Coronavirus Austr...|          Positive|[coronavirus, aus...|[coronavirus, aus...|(1000,[0,5,6,8,9,...|  0.0|\n",
      "|As news of the re...|          Positive|[as, news, of, th...|[as, news, of, re...|(1000,[0,1,2,5,8,...|  0.0|\n",
      "|\"Cashier at groce...|          Positive|[cashier, at, gro...|[cashier, at, gro...|(1000,[0,4,5,11,1...|  0.0|\n",
      "|Due to COVID-19 o...|          Positive|[due, to, covid, ...|[due, to, covid, ...|(1000,[0,1,4,5,7,...|  0.0|\n",
      "|For corona preven...|          Negative|[for, corona, pre...|[for, corona, pre...|(1000,[0,1,7,8,9,...|  1.0|\n",
      "|All month there h...|           Neutral|[all, month, ther...|[all, month, ther...|(1000,[1,3,4,5,10...|  2.0|\n",
      "|#horningsea is a ...|Extremely Positive|[horningsea, is, ...|[horningsea, is, ...|(1000,[0,1,3,4,5,...|  3.0|\n",
      "|ADARA Releases CO...|          Positive|[adara, releases,...|[adara, releases,...|(1000,[0,5,7,8,9,...|  0.0|\n",
      "+--------------------+------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "label_stringIdx = StringIndexer(inputCol = \"Sentiment\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
    "pipelineFit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)\n",
    "dataset.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Count: 20996\n",
      "Test Data Count: 6966\n"
     ]
    }
   ],
   "source": [
    "(trainData, testData) = dataset.randomSplit([0.75, 0.25], seed=120)\n",
    "print(\"Training Data Count: \" + str(trainData.count()))\n",
    "print(\"Test Data Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Random Forest Classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+------------------+------------------------------+-----+----------+\n",
      "|                 OriginalTweet|         Sentiment|                   probability|label|prediction|\n",
      "+------------------------------+------------------+------------------------------+-----+----------+\n",
      "|All e com sites like @amazo...|Extremely Positive|[0.3234730543809337,0.20512...|  3.0|       0.0|\n",
      "|looking forward to working ...|Extremely Positive|[0.3225341763305235,0.18365...|  3.0|       0.0|\n",
      "|To help contain the COVID-1...|Extremely Positive|[0.32141291519207366,0.1847...|  3.0|       0.0|\n",
      "|The first 2 steps in the di...|Extremely Positive|[0.3185136475272952,0.20021...|  3.0|       0.0|\n",
      "|Today At Riverside HS I ran...|Extremely Positive|[0.3181401306110266,0.20709...|  3.0|       0.0|\n",
      "|Like for Hand sanitizer Ret...|Extremely Positive|[0.31798576146088153,0.2054...|  3.0|       0.0|\n",
      "|Brewers and distillers acro...|          Positive|[0.3178164717030156,0.19192...|  0.0|       0.0|\n",
      "|has found out that exports ...|Extremely Positive|[0.3175385984543492,0.20387...|  3.0|       0.0|\n",
      "|@evanrbatt I saw what youÂ’r...|Extremely Positive|[0.31745975024308726,0.1803...|  3.0|       0.0|\n",
      "|I just got hand sanitizer a...|Extremely Positive|[0.3171744204487443,0.20440...|  3.0|       0.0|\n",
      "+------------------------------+------------------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(labelCol=\"label\", \n",
    "                            featuresCol=\"features\",\n",
    "                            numTrees = 100, \n",
    "                            maxDepth = 4, \n",
    "                            maxBins = 32)\n",
    "# Training the model\n",
    "rfcModel = rfc.fit(trainData)\n",
    "# Making prediction\n",
    "predictions = rfcModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) .select(\"OriginalTweet\",\"Sentiment\",\"probability\",\"label\",\"prediction\") .orderBy(\"probability\", ascending=False)  .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1770125486389085"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the accuracy of model\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+------------------+------------------------------+-----+----------+\n",
      "|                 OriginalTweet|         Sentiment|                   probability|label|prediction|\n",
      "+------------------------------+------------------+------------------------------+-----+----------+\n",
      "|If you want to learn about ...|          Positive|[0.6246888881907727,0.07304...|  0.0|       0.0|\n",
      "|The UK Government is helpin...|Extremely Positive|[0.6091860852243968,0.06411...|  3.0|       0.0|\n",
      "|Members - Tune in Wed for a...|Extremely Positive|[0.6024668771197693,0.03902...|  3.0|       0.0|\n",
      "|@adrparsons @commerson @Mic...|          Positive|[0.5937497976647509,0.16216...|  0.0|       0.0|\n",
      "|Please join us for a webina...|Extremely Positive|[0.5930786297371786,0.04188...|  3.0|       0.0|\n",
      "|Appalachian Wireless has le...|          Positive|[0.5882698052413038,0.11207...|  0.0|       0.0|\n",
      "|Last week, @Captify opened ...|          Positive|[0.576844022207999,0.098579...|  0.0|       0.0|\n",
      "|Important Notice In order t...|Extremely Positive|[0.5760670056351964,0.07662...|  3.0|       0.0|\n",
      "|Wuhan has opened the The to...|          Negative|[0.5738369161535873,0.10149...|  1.0|       0.0|\n",
      "|Hey why r we still waiting ...|Extremely Positive|[0.57096516169446,0.0914726...|  3.0|       0.0|\n",
      "+------------------------------+------------------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "logrModel = logr.fit(trainData)\n",
    "# making prediction on test data \n",
    "predictions = logrModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) .select(\"OriginalTweet\",\"Sentiment\",\"probability\",\"label\",\"prediction\") .orderBy(\"probability\", ascending=False) .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4841331688930419"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the accuracy of model\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that Logistic Regression model gives the best accuracy for the dataset i.e., 48% as compared to Random forest classifier model i.e., around 18%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
